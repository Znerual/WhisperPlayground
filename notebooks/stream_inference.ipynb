{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/nalbion/whisper-server/blob/master/whisper_server/services/audio/microphone.py\n",
    "\n",
    "import numpy as np\n",
    "import pyaudio\n",
    "import time\n",
    "import logging \n",
    "import scipy\n",
    "\n",
    "import wave\n",
    "import librosa\n",
    "import queue\n",
    "import signal\n",
    "from multiprocessing import Process, Queue, Event\n",
    "from threading import Thread\n",
    "\n",
    "from faster_whisper import WhisperModel\n",
    "from faster_whisper.transcribe import Segment\n",
    "\n",
    "CHUNK_LENGTH = 30  # seconds\n",
    "\n",
    "SAMPLE_RATE = 16000# 16000\n",
    "RECORD_SECONDS = CHUNK_LENGTH  # 30\n",
    "# TODO: bring FRAMES_PER_BUFFER down and see if it breaks anything or improves latency\n",
    "# FRAMES_PER_BUFFER = 1024\n",
    "# FRAMES_PER_BUFFER = 3000\n",
    "# OpenAI Whisper complains if not 480000: assert x.shape[1:] == self.positional_embedding.shape, \"incorrect audio shape\"\n",
    "# FRAMES_PER_BUFFER = RECORD_SECONDS * SAMPLE_RATE  # 480,000 = N_SAMPLES\n",
    "# FRAMES_TO_PROCESS = FRAMES_PER_BUFFER >> 3\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 1\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class Microphone:\n",
    "    def __init__(self, device_index=None):\n",
    "        self.closed = False\n",
    "        self.audio = pyaudio.PyAudio()\n",
    "        self.run = False\n",
    "        self.device_index = device_index\n",
    "        self.device_sample_rate = None\n",
    "        self.list_input_devices()\n",
    "        self.frames_to_process = (RECORD_SECONDS * self.device_sample_rate) >> 1\n",
    "        print(f\"Use device index: {self.device_index} with sample rate {self.device_sample_rate}\")\n",
    "        self.stream = self.audio.open(format=FORMAT,\n",
    "                                      channels=CHANNELS,\n",
    "                                      rate=self.device_sample_rate,\n",
    "                                      input=True,\n",
    "                                      input_device_index=self.device_index,\n",
    "                                      frames_per_buffer=RECORD_SECONDS * self.device_sample_rate)\n",
    "        \n",
    "        self.output_wave = wave.open(\"test.wav\", 'wb')\n",
    "        self.output_wave.setnchannels(1)\n",
    "        self.output_wave.setsampwidth(self.audio.get_sample_size(FORMAT))\n",
    "        self.output_wave.setframerate(SAMPLE_RATE)\n",
    "\n",
    "    def __exit__(self, *err):\n",
    "        self.close()\n",
    "\n",
    "    def __del__(self):\n",
    "        self.close()\n",
    "\n",
    "    def list_input_devices(self):\n",
    "\n",
    "        hosts = self.audio.get_host_api_count()\n",
    "        device_count = self.audio.get_device_count()\n",
    "\n",
    "        for index in range(hosts):\n",
    "            host = self.audio.get_host_api_info_by_index(index)\n",
    "            print(\"-----------------------\")\n",
    "            print(f\"Host audio API {index}: {host['name']}\")\n",
    "\n",
    "            for d in range(device_count):\n",
    "                device_info = self.audio.get_device_info_by_index(d)\n",
    "                inputs = device_info['maxInputChannels']\n",
    "\n",
    "                try:\n",
    "                     if 1 <= inputs <= 2 and device_info['hostApi'] == index: # and \\\n",
    "                    #         self.audio.is_format_supported(rate=SAMPLE_RATE,\n",
    "                    #                                        input_device=d,\n",
    "                    #                                        input_format=FORMAT,\n",
    "                    #                                        input_channels=1):\n",
    "                        print(str(device_info))\n",
    "                        print(f\"  Device {d}: {device_info['name']}\")\n",
    "                        if self.device_index is None:\n",
    "                            self.device_index = d\n",
    "                            self.device_sample_rate = int(device_info['defaultSampleRate'])\n",
    "                            print(f\"Use Device {device_info['name']}\")\n",
    "                        elif self.device_index == d:\n",
    "                            self.device_sample_rate = int(device_info['defaultSampleRate'])\n",
    "                            print(f\"Use Device {device_info['name']}\")\n",
    "                except ValueError:\n",
    "                    # print(str(device_info))\n",
    "                    pass\n",
    "\n",
    "\n",
    "\n",
    "        # for index in range(device_count):\n",
    "        #     device_info = self.audio.get_device_info_by_index(index)\n",
    "        #\n",
    "        #     try:\n",
    "        #         if device_info['maxInputChannels'] == 1 and \\\n",
    "        #             device_info['hostApi'] == 0 and \\\n",
    "        #             self.audio.is_format_supported(rate=SAMPLE_RATE,\n",
    "        #                                           input_device=index,\n",
    "        #                                           input_format=FORMAT,\n",
    "        #                                           input_channels=1):\n",
    "        #             print(str(device_info))\n",
    "        #             # print(f\"Device {index}: {device_info['name']}\")\n",
    "        #     except ValueError:\n",
    "        #         # print(str(device_info))\n",
    "        #         pass\n",
    "\n",
    "\n",
    "\n",
    "    def listen(self):\n",
    "        while self.run:\n",
    "            prev = time.time()\n",
    "            if self.stream.is_active():\n",
    "                start = time.time()\n",
    "                # print(\"listening...\")  # {:.3f}\".format(time.time() - start))\n",
    "                data = self.stream.read(self.frames_to_process)\n",
    "                # print(\"got audio from the mic, {:.3f}\".format(time.time() - start))\n",
    "                prev = time.time()\n",
    "                audio_data = np.frombuffer(data, np.int16).flatten().astype(np.float32) / 32768.0\n",
    "              \n",
    "                if self.device_sample_rate != SAMPLE_RATE:\n",
    "                    audio_data = scipy.signal.resample_poly(audio_data, SAMPLE_RATE, self.device_sample_rate)\n",
    "            \n",
    "                    #audio_data = librosa.resample(audio_data, orig_sr=self.device_sample_rate, target_sr=SAMPLE_RATE)\n",
    "                self.output_wave.writeframes((audio_data * 32768).astype(np.int16).tobytes())\n",
    "                    \n",
    "                \n",
    "                \n",
    "                yield audio_data\n",
    "                # print(\"after yield, {:.3f}\".format(time.time() - prev))\n",
    "            else:\n",
    "                print(\"break from microphone.listen()\")\n",
    "                break\n",
    "\n",
    "    def start(self):\n",
    "        self.run = True\n",
    "        self.stream.start_stream()\n",
    "        logger.debug(\"microphone started\")\n",
    "\n",
    "    def stop(self):\n",
    "        logger.debug(\"microphone stopped\")\n",
    "        self.run = False\n",
    "        self.stream.stop_stream()\n",
    "        self.output_wave.close()\n",
    "\n",
    "    def close(self):\n",
    "        if not self.closed:\n",
    "            self.stream.stop_stream()\n",
    "            self.stream.close()\n",
    "            self.output_wave.close()\n",
    "            self.audio.terminate()\n",
    "            self.closed = True\n",
    "\n",
    "    def is_closed(self):\n",
    "        return self.closed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def start_audio_process(audio_queue: Queue, audio_active_event: Event, device_index:int = None):\n",
    "    mic = Microphone(device_index=device_index)\n",
    "\n",
    "    if audio_active_event.is_set():\n",
    "        mic.start()\n",
    "\n",
    "    def process_record_audio():  # audio_queue: multiprocessing.Queue):\n",
    "        while not mic.is_closed():\n",
    "            if not audio_active_event.is_set():\n",
    "                mic.stop()\n",
    "                audio_active_event.wait()\n",
    "                mic.start()\n",
    "\n",
    "            for audio in mic.listen():\n",
    "                if audio_active_event.is_set():\n",
    "                    audio_queue.put(audio)\n",
    "                    print(\"Wrote audio to queue\")\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "    audio_process = Process(name=\"whisper_server audio\", target=process_record_audio)  # , args=(audio_queue,))\n",
    "    audio_process.start()\n",
    "    \n",
    "    async def stop():\n",
    "        mic.stop()\n",
    "        mic.close()\n",
    "        audio_queue.close()\n",
    "        audio_queue.cancel_join_thread()\n",
    "        audio_process.terminate()\n",
    "        audio_process.join()\n",
    "\n",
    "    return stop\n",
    "\n",
    "\n",
    "def start_audio_thread(audio_queue: Queue, audio_active_event: Event, device_index:int = None):\n",
    "    mic = Microphone(device_index=device_index)\n",
    "\n",
    "    if audio_active_event.is_set():\n",
    "        mic.start()\n",
    "\n",
    "    def run_audio_loop():\n",
    "        while not mic.is_closed():\n",
    "            if not audio_active_event.is_set():\n",
    "                mic.stop()\n",
    "                # print(\"run_audio_loop waiting for startRecognition()\")\n",
    "                audio_active_event.wait()\n",
    "                if mic.is_closed():\n",
    "                    break\n",
    "                mic.start()\n",
    "\n",
    "            for audio in mic.listen():\n",
    "                if audio_active_event.is_set():\n",
    "                    audio_queue.put(audio)\n",
    "                    print(\"Wrote audio to queue\")\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "    audio_thread = Thread(name=\"whisper_server audio\", target=run_audio_loop)\n",
    "    audio_thread.start()\n",
    "\n",
    "    async def stop():\n",
    "        mic.stop()\n",
    "        mic.close()\n",
    "        audio_active_event.clear()\n",
    "        audio_queue.close()\n",
    "        audio_queue.cancel_join_thread()\n",
    "        audio_thread.join()\n",
    "\n",
    "    return stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def signal_handler(interrupted_event):\n",
    "    interrupted_event.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_results(self, hypothesis: Segment):\n",
    "        # Ignore no_speech_prob >= 0.75\n",
    "        # avg_logprob:\n",
    "        #  >= -0.3 : good\n",
    "        #     -0.4 : close\n",
    "        #  >  -0.8 : clear speech mis-recognised or\n",
    "        #  <= -0.8 : mis-pronounced\n",
    "        #  <  -1.0 : is mumbled/hard to hear\n",
    "        # print('avg_logprob: {:.3f}, no_speech_prob: {:.3f}'.format(hypothesis.avg_logprob, hypothesis.no_speech_prob))\n",
    "        logger.debug('avg_logprob: %.3f, no_speech_prob: %.3f', hypothesis.avg_logprob, hypothesis.no_speech_prob)\n",
    "        return hypothesis.no_speech_prob < 0.75 and hypothesis.avg_logprob > -1.0\n",
    "\n",
    "def run_whisper_loop(args, _audio_queue: Queue, _stt_results_queue: Queue, interrupted_event: Event):\n",
    "  \n",
    "    model = WhisperModel(\"../quantized_models/whisper-large-v2\", device=\"cpu\", compute_type=\"int8\")\n",
    "    signal.signal(signal.SIGINT, lambda sig, frame: signal_handler(interrupted_event))\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            audio = _audio_queue.get()\n",
    "            # if audio == \"stop\":\n",
    "            #     print(\"process_speech_to_text received 'stop'\")\n",
    "            #     break\n",
    "            # logger.debug(\"sending audio to Whisper...\")\n",
    "            segments, _ = model.transcribe(audio, word_timestamps=False, vad_filter=True, vad_parameters=dict(min_silence_duration_ms=1000))\n",
    "            alternatives = list(segments)\n",
    "            print(f\"Whisper returned {alternatives}\")\n",
    "            #alternatives = filter(filter_results, segments)\n",
    "            if len(alternatives) != 0:\n",
    "                logger.info(\"Whisper recognised: %s\", alternatives)\n",
    "                _stt_results_queue.put(alternatives)\n",
    "        except ValueError:\n",
    "            print(\"  ValueError in reading from audio_queue\")\n",
    "            break\n",
    "        except queue.Empty:\n",
    "            print(\"empty audio queue\")\n",
    "            break\n",
    "        except (KeyboardInterrupt, InterruptedError, SystemExit):\n",
    "            _audio_queue.close()\n",
    "            _audio_queue.cancel_join_thread()\n",
    "            _stt_results_queue.close()\n",
    "            _stt_results_queue.cancel_join_thread()\n",
    "            print(\"---------- whisper process interrupted by keyboard ---------\")\n",
    "            break\n",
    "        \n",
    "def start_whisper_process(args, audio_queue: Queue, stt_results_queue: Queue, interrupted_event: Event):\n",
    "    # interrupted_event = Event()\n",
    "    whisper_process = Process(name=\"whisper_server speech to text\",\n",
    "                              target=run_whisper_loop,\n",
    "                              args=(args, audio_queue, stt_results_queue, interrupted_event),\n",
    "                              # daemon=True\n",
    "                              )\n",
    "    whisper_process.start()\n",
    "\n",
    "    async def stop():\n",
    "        # terminate is more graceful than kill (abort)\n",
    "        # audio_queue.put(\"stop\")\n",
    "        audio_queue.close()\n",
    "        audio_queue.cancel_join_thread()\n",
    "        stt_results_queue.close()\n",
    "        stt_results_queue.cancel_join_thread()\n",
    "        whisper_process.terminate()\n",
    "        whisper_process.join()\n",
    "\n",
    "    return stop\n",
    "\n",
    "\n",
    "def start_whisper_thread(args, audio_queue: Queue, stt_results_queue: Queue, interrupted_event: Event):\n",
    "    whisper_thread = Thread(name=\"whisper_server speech to text\",\n",
    "                            target=run_whisper_loop,\n",
    "                            args=(args, audio_queue, stt_results_queue, interrupted_event),)\n",
    "    whisper_thread.start()\n",
    "\n",
    "    async def stop():\n",
    "        # terminate is more graceful than kill (abort)\n",
    "        # audio_queue.put(\"stop\")\n",
    "        audio_queue.close()\n",
    "        audio_queue.cancel_join_thread()\n",
    "        stt_results_queue.close()\n",
    "        stt_results_queue.cancel_join_thread()\n",
    "        whisper_thread.join()\n",
    "\n",
    "    return stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------\n",
      "Host audio API 0: ALSA\n",
      "{'index': 0, 'structVersion': 2, 'name': 'HDA Intel PCH: ALC3235 Analog (hw:0,0)', 'hostApi': 0, 'maxInputChannels': 2, 'maxOutputChannels': 0, 'defaultLowInputLatency': 0.008707482993197279, 'defaultLowOutputLatency': -1.0, 'defaultHighInputLatency': 0.034829931972789115, 'defaultHighOutputLatency': -1.0, 'defaultSampleRate': 44100.0}\n",
      "  Device 0: HDA Intel PCH: ALC3235 Analog (hw:0,0)\n",
      "{'index': 13, 'structVersion': 2, 'name': 'HD Pro Webcam C920: USB Audio (hw:2,0)', 'hostApi': 0, 'maxInputChannels': 2, 'maxOutputChannels': 0, 'defaultLowInputLatency': 0.012, 'defaultLowOutputLatency': -1.0, 'defaultHighInputLatency': 0.048, 'defaultHighOutputLatency': -1.0, 'defaultSampleRate': 32000.0}\n",
      "  Device 13: HD Pro Webcam C920: USB Audio (hw:2,0)\n",
      "Use Device HD Pro Webcam C920: USB Audio (hw:2,0)\n",
      "-----------------------\n",
      "Host audio API 1: OSS\n",
      "Use device index: 13 with sample rate 32000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ALSA lib pcm_dmix.c:1089:(snd_pcm_dmix_open) unable to open slave\n",
      "ALSA lib pcm.c:2642:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.rear\n",
      "ALSA lib pcm.c:2642:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.center_lfe\n",
      "ALSA lib pcm.c:2642:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.side\n",
      "ALSA lib pcm_route.c:869:(find_matching_chmap) Found no matching channel map\n",
      "Cannot connect to server socket err = No such file or directory\n",
      "Cannot connect to server request channel\n",
      "jack server is not running or cannot be started\n",
      "JackShmReadWritePtr::~JackShmReadWritePtr - Init not done for -1, skipping unlock\n",
      "JackShmReadWritePtr::~JackShmReadWritePtr - Init not done for -1, skipping unlock\n",
      "Cannot connect to server socket err = No such file or directory\n",
      "Cannot connect to server request channel\n",
      "jack server is not running or cannot be started\n",
      "JackShmReadWritePtr::~JackShmReadWritePtr - Init not done for -1, skipping unlock\n",
      "JackShmReadWritePtr::~JackShmReadWritePtr - Init not done for -1, skipping unlock\n",
      "ALSA lib pcm_oss.c:377:(_snd_pcm_oss_open) Unknown field port\n",
      "ALSA lib pcm_oss.c:377:(_snd_pcm_oss_open) Unknown field port\n",
      "ALSA lib pcm_usb_stream.c:486:(_snd_pcm_usb_stream_open) Invalid type for card\n",
      "ALSA lib pcm_usb_stream.c:486:(_snd_pcm_usb_stream_open) Invalid type for card\n",
      "ALSA lib pcm_dmix.c:1089:(snd_pcm_dmix_open) unable to open slave\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote audio to queue\n",
      "Wrote audio to queue\n",
      "Whisper returned [Segment(id=1, seek=1108, start=3.57, end=13.92, text=' Schauen wir mal, jetzt sollten die Abstände etwas länger werden, die Audiosachen sollten noch seltener geschrieben werden.', tokens=[50364, 2065, 11715, 1987, 2806, 11, 4354, 29096, 978, 2847, 16913, 68, 9569, 40935, 4604, 11, 978, 8821, 2717, 11646, 29096, 3514, 5851, 1147, 260, 47397, 4604, 13, 50864], temperature=0.0, avg_logprob=-0.45756151477495827, compression_ratio=1.203883495145631, no_speech_prob=0.031490862369537354, words=None)]\n",
      "Wrote audio to queue\n",
      "Whisper returned [Segment(id=1, seek=1500, start=0.0, end=5.04, text=' Größenordnungen seltener. Dafür habe ich die Hoffnung, dass vielleicht die Qualität besser', tokens=[50364, 45778, 8989, 765, 77, 5084, 5851, 1147, 260, 13, 35865, 6015, 1893, 978, 29135, 15539, 11, 2658, 12547, 978, 13616, 14053, 18021, 50616], temperature=0.0, avg_logprob=-0.29906214200533354, compression_ratio=1.3224043715846994, no_speech_prob=0.051410745829343796, words=None), Segment(id=2, seek=1500, start=5.04, end=10.08, text=' ist, weil das Modell mehr Daten hat. Das wird jetzt interessant zu sehen, wenn das die erste', tokens=[50616, 1418, 11, 7689, 1482, 6583, 898, 5417, 31126, 2385, 13, 2846, 4578, 4354, 37748, 2164, 11333, 11, 4797, 1482, 978, 20951, 50868], temperature=0.0, avg_logprob=-0.29906214200533354, compression_ratio=1.3224043715846994, no_speech_prob=0.051410745829343796, words=None), Segment(id=3, seek=1500, start=10.08, end=14.96, text=' Rückmeldung zurückkommt vom Modell. Und ich damit...', tokens=[50868, 35001, 76, 5957, 1063, 15089, 74, 22230, 10135, 6583, 898, 13, 2719, 1893, 9479, 485, 51112], temperature=0.0, avg_logprob=-0.29906214200533354, compression_ratio=1.3224043715846994, no_speech_prob=0.051410745829343796, words=None)]\n",
      "Wrote audio to queue\n",
      "Whisper returned [Segment(id=1, seek=1381, start=0.0, end=10.18, text=' Dann erkenne, schauen wir mal, jetzt sollten die Abstände etwas länger werden, die Audio-Sachen sollten noch seltener beschrieben werden.', tokens=[50364, 7455, 1189, 2653, 716, 11, 25672, 1987, 2806, 11, 4354, 29096, 978, 2847, 16913, 68, 9569, 40935, 4604, 11, 978, 25706, 12, 50, 11646, 29096, 3514, 5851, 1147, 260, 17498, 24027, 4604, 13, 50814], temperature=0.0, avg_logprob=-0.44692088575924144, compression_ratio=1.326241134751773, no_speech_prob=0.046608392149209976, words=None), Segment(id=2, seek=1381, start=10.18, end=14.18, text=' Ja, okay, das ist schon mal ein längerer Text.', tokens=[50814, 3530, 11, 1392, 11, 1482, 1418, 4981, 2806, 1343, 22566, 29566, 18643, 13, 51014], temperature=0.0, avg_logprob=-0.44692088575924144, compression_ratio=1.326241134751773, no_speech_prob=0.046608392149209976, words=None)]\n",
      "Wrote audio to queue\n",
      "Wrote audio to queue\n",
      "Wrote audio to queue\n",
      "Wrote audio to queue\n",
      "Wrote audio to queue\n",
      "Wrote audio to queue\n",
      "Wrote audio to queue\n",
      "Wrote audio to queue\n"
     ]
    }
   ],
   "source": [
    "interrupted_event = Event()\n",
    "audio_active_event = Event()\n",
    "audio_active_event.set()\n",
    "\n",
    "\n",
    "audio_queue = Queue()\n",
    "stt_results_queue = Queue()\n",
    "\n",
    "audio_stop = start_audio_process(audio_queue=audio_queue, audio_active_event=audio_active_event, device_index=13)\n",
    "whisper_stop = start_whisper_process(args=None, audio_queue=audio_queue, stt_results_queue=stt_results_queue, interrupted_event=interrupted_event)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "Empty",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mprint\u001b[39m(audio_queue\u001b[39m.\u001b[39;49mget(block\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m))\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/multiprocessing/queues.py:109\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    107\u001b[0m     deadline \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mmonotonic() \u001b[39m+\u001b[39m timeout\n\u001b[1;32m    108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_rlock\u001b[39m.\u001b[39macquire(block, timeout):\n\u001b[0;32m--> 109\u001b[0m     \u001b[39mraise\u001b[39;00m Empty\n\u001b[1;32m    110\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    111\u001b[0m     \u001b[39mif\u001b[39;00m block:\n",
      "\u001b[0;31mEmpty\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(audio_queue.get(block=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "Empty",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mprint\u001b[39m(stt_results_queue\u001b[39m.\u001b[39;49mget(block\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m))\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/multiprocessing/queues.py:116\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[39mraise\u001b[39;00m Empty\n\u001b[1;32m    115\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_poll():\n\u001b[0;32m--> 116\u001b[0m     \u001b[39mraise\u001b[39;00m Empty\n\u001b[1;32m    117\u001b[0m res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_recv_bytes()\n\u001b[1;32m    118\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sem\u001b[39m.\u001b[39mrelease()\n",
      "\u001b[0;31mEmpty\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(stt_results_queue.get(block=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "await whisper_stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread whisper_server audio:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "Expression 'alsa_snd_pcm_poll_descriptors_revents( self->pcm, pfds, self->nfds, &revents )' failed in 'src/hostapi/alsa/pa_linux_alsa.c', line: 3664\n",
      "Expression 'PaAlsaStreamComponent_EndPolling( &self->capture, capturePfds, &pollCapture, &xrun )' failed in 'src/hostapi/alsa/pa_linux_alsa.c', line: 3891\n",
      "Expression 'PaAlsaStream_WaitForFrames( stream, &framesAvail, &xrun )' failed in 'src/hostapi/alsa/pa_linux_alsa.c', line: 4438\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.10/threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/tmp/ipykernel_13729/2299478538.py\", line 49, in run_audio_loop\n",
      "  File \"/tmp/ipykernel_13729/1389239608.py\", line 106, in listen\n",
      "  File \"/home/ruzickal/Code/Privat/Whisper/whisper_venv/lib/python3.10/site-packages/pyaudio/__init__.py\", line 570, in read\n",
      "    return pa.read_stream(self._stream, num_frames,\n",
      "OSError: [Errno -9999] Unanticipated host error\n"
     ]
    }
   ],
   "source": [
    "await audio_stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whisper_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
